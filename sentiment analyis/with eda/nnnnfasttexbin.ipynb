{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import fasttext\n",
    "import certifi\n",
    "import ssl\n",
    "import urllib.request\n",
    "import os\n",
    "import gzip\n",
    "import spacy\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, classification_report\n",
    "import subprocess\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold,RandomizedSearchCV\n",
    "\n",
    "# seed = 2024\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each label_sentiment:\n",
      "sentiment\n",
      "Negative    262\n",
      "Neither      99\n",
      "Positive     48\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions of each label_sentiment:\n",
      "sentiment\n",
      "Negative    0.640587\n",
      "Neither     0.242054\n",
      "Positive    0.117359\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('/Users/rachael/Downloads/train_df_labelled copy.xlsx')\n",
    "#the data verified by the third person will be used as the train set finally\n",
    "file =  data[['content','sentiment']]\n",
    "file['sentiment'].value_counts()\n",
    "label_sentiment= file['sentiment'].value_counts()\n",
    "label_proportions_sentiment= file['sentiment'].value_counts(normalize=True)\n",
    "print(\"Counts of each label_sentiment:\")\n",
    "print(label_sentiment)\n",
    "print(\"\\nProportions of each label_sentiment:\")\n",
    "print(label_proportions_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls(text):\n",
    "    # the expression of URL_pattern\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    # use re.sub()to sustitude URL with blank str\n",
    "    no_url_text = re.sub(url_pattern, '', text)\n",
    "    return no_url_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove anything that is not a letter or space\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Optional: Convert text to lower case\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_7650/280474677.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['content'] = file['content'].apply(remove_urls)\n",
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_7650/280474677.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['content'] = file['content'].apply(clean_text)\n",
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_7650/280474677.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['sentiment'] = file['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n"
     ]
    }
   ],
   "source": [
    "#clean the data\n",
    "file['content'] = file['content'].apply(remove_urls) \n",
    "file['content'] = file['content'].apply(clean_text) \n",
    "\n",
    "#convert categorical to numerical \n",
    "file['sentiment'] = file['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n",
    "\n",
    "# Load dataset\n",
    "y = file['sentiment']\n",
    "X = file['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyICU\n",
      "  Downloading PyICU-2.13.1.tar.gz (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: PyICU\n",
      "  Building wheel for PyICU (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyICU: filename=PyICU-2.13.1-cp311-cp311-macosx_10_9_universal2.whl size=708994 sha256=f5f1e81865f2eabd714a5ce100c9e46739bb0a4f3b29125bece02c7c3f57910f\n",
      "  Stored in directory: /Users/rachael/Library/Caches/pip/wheels/f7/40/2e/f5fa8292cf2624cd36da4b648670f6bd72132d75a953561e02\n",
      "Successfully built PyICU\n",
      "Installing collected packages: PyICU\n",
      "Successfully installed PyICU-2.13.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m342.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2\n",
      "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/rachael/Library/Python/3.11/lib/python/site-packages (from fasttext) (69.0.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fasttext) (1.26.1)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp311-cp311-macosx_13_0_universal2.whl size=670562 sha256=ffdde8db49c7e3a83f21f77a4d70121ca724ffecebb52e74ec7dcbbb39fe6889\n",
      "  Stored in directory: /Users/rachael/Library/Caches/pip/wheels/9c/42/d0/359c8cbd57e79a065dc3acf282270fccc57fdba07b307a299b\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "\u001b[33m  WARNING: The script pybind11-config is installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed fasttext-0.9.2 pybind11-2.12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Path to your FastText pre-trained word embedding file\n",
    "fasttext_model_path = '/Users/rachael/Downloads/cc.nl.300.bin'\n",
    "\n",
    "# Load the FastText model\n",
    "ft = fasttext.load_model(fasttext_model_path)\n",
    "\n",
    "# Example usage: Get the vector for a word\n",
    "# word = \"voorbeeld\"\n",
    "# word_vector = ft.get_word_vector(word)\n",
    "\n",
    "# print(f\"Vector for the word '{word}':\\n{word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n",
      "Name: PyICU\n",
      "Version: 2.13.1\n",
      "Summary: Python extension wrapping the ICU C++ API\n",
      "Home-page: https://gitlab.pyicu.org/main/pyicu\n",
      "Author: Andi Vajda\n",
      "Author-email: vajda@pyicu.org\n",
      "License: MIT\n",
      "Location: /Users/rachael/opt/anaconda3/lib/python3.9/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!pip show PyICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: brew\n"
     ]
    }
   ],
   "source": [
    "!brew install icu4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "#tokenize\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def sentence_to_avg_vec(tokens, model):\n",
    "    vector_size = model.get_dimension()\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        vec += model.get_word_vector(token)\n",
    "        count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist= {\n",
    "    'n_estimators': range(10, 101),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2, 51),\n",
    "    'min_samples_split': range(2, 11),\n",
    "    'min_samples_leaf': range(1, 11),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "test_scores = []\n",
    "best_params_list = []\n",
    "\n",
    "# Stratified K-fold for maintaining label distribution,shuffle=True确保每次迭代的数据划分不同\n",
    "# Initialize parameters\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=9\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=1\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=4\n",
      "Fold F1-score: 0.34488888888888886\n",
      "Best parameters: {'n_estimators': 90, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'gini', 'bootstrap': False}\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=9\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=1\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=4\n",
      "Fold F1-score: 0.3641499743194659\n",
      "Best parameters: {'n_estimators': 48, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 39, 'criterion': 'entropy', 'bootstrap': False}\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=9\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=1\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=4\n",
      "Fold F1-score: 0.3936144663417391\n",
      "Best parameters: {'n_estimators': 46, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 28, 'criterion': 'gini', 'bootstrap': False}\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=9\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=1\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=4\n",
      "Fold F1-score: 0.3816091954022989\n",
      "Best parameters: {'n_estimators': 56, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 28, 'criterion': 'entropy', 'bootstrap': False}\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=9\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=1\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=4\n",
      "Fold F1-score: 0.29152148664343785\n",
      "Best parameters: {'n_estimators': 77, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 41, 'criterion': 'gini', 'bootstrap': False}\n",
      "Mean F1-score: 0.35515680231916613\n",
      "Standard deviation of F1-scores: 0.03581754210407493\n",
      "Best parameters for each fold:\n",
      "Fold 1: {'n_estimators': 90, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'gini', 'bootstrap': False}\n",
      "Fold 2: {'n_estimators': 48, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 39, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Fold 3: {'n_estimators': 46, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 28, 'criterion': 'gini', 'bootstrap': False}\n",
      "Fold 4: {'n_estimators': 56, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 28, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Fold 5: {'n_estimators': 77, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 41, 'criterion': 'gini', 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    trainX, testX = X.iloc[train_index], X.iloc[test_index]\n",
    "    trainy, testy = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # convert y_train and y_test as DataFrame\n",
    "    y_train_df = pd.DataFrame(trainy).reset_index(drop=True)\n",
    "    y_test_df = pd.DataFrame(testy).reset_index(drop=True)\n",
    "\n",
    "    # concat X_train and y_train, X_test and y_test\n",
    "    train_df = pd.concat([trainX.reset_index(drop=True), y_train_df], axis=1)\n",
    "    test_df = pd.concat([testX.reset_index(drop=True), y_test_df], axis=1)\n",
    "\n",
    "    #prepare the data for data augmentation\n",
    "    train_df['content'] = train_df['content'].apply(lambda x: ' '.join(x.split()))\n",
    "    sentiment_0 = train_df[train_df['sentiment'] == 0]\n",
    "    sentiment_1 = train_df[train_df['sentiment'] == 1]\n",
    "    sentiment_2 = train_df[train_df['sentiment'] == 2]\n",
    "\n",
    "    # the path to preserve prepared data for EDA data augmentation for class0 \n",
    "    output_file_path0 = '/Users/rachael/Desktop/data/sentiment_0.txt'\n",
    "    #convert DataFrame data into a simple tab-separated text file format，required by the data augmentation technique\n",
    "    with open(output_file_path0, 'w', encoding='utf-8') as f:\n",
    "        for _, row in sentiment_0.iterrows():\n",
    "            f.write(f\"{row['sentiment']}\\t{row['content']}\\n\")\n",
    "    subprocess.run([\n",
    "    'python', '/Users/rachael/Desktop/data/augment.py',\n",
    "    '--input', output_file_path0,\n",
    "    '--output', '/Users/rachael/Desktop/data/augmented_train0.txt',  # Output as txt if required\n",
    "    '--num_aug', '9',\n",
    "    '--alpha_sr', '0.1',\n",
    "    '--alpha_rd', '0.1',\n",
    "    '--alpha_ri', '0.1',\n",
    "    '--alpha_rs', '0.1'\n",
    "])\n",
    "\n",
    "    #for class1\n",
    "    output_file_path1 = '/Users/rachael/Desktop/data/sentiment_1.txt'\n",
    "    with open(output_file_path1, 'w', encoding='utf-8') as f:\n",
    "        for _, row in sentiment_1.iterrows():\n",
    "            f.write(f\"{row['sentiment']}\\t{row['content']}\\n\") \n",
    "    subprocess.run([\n",
    "    'python', '/Users/rachael/Desktop/data/augment.py',\n",
    "    '--input', output_file_path1,\n",
    "    '--output', '/Users/rachael/Desktop/data/augmented_train1.txt',  # Output as txt if required\n",
    "    '--num_aug', '1',\n",
    "    '--alpha_sr', '0.1',\n",
    "    '--alpha_rd', '0.1',\n",
    "    '--alpha_ri', '0.1',\n",
    "    '--alpha_rs', '0.1'\n",
    "])\n",
    "    \n",
    "    #for class2\n",
    "    output_file_path2 = '/Users/rachael/Desktop/data/sentiment_2.txt'\n",
    "    with open(output_file_path2, 'w', encoding='utf-8') as f:\n",
    "        for _, row in sentiment_2.iterrows():\n",
    "            f.write(f\"{row['sentiment']}\\t{row['content']}\\n\")  \n",
    "    subprocess.run([\n",
    "    'python', '/Users/rachael/Desktop/data/augment.py',\n",
    "    '--input', output_file_path2,\n",
    "    '--output', '/Users/rachael/Desktop/data/augmented_train2.txt',  # Output as txt if required\n",
    "    '--num_aug', '4',\n",
    "    '--alpha_sr', '0.1',\n",
    "    '--alpha_rd', '0.1',\n",
    "    '--alpha_ri', '0.1',\n",
    "    '--alpha_rs', '0.1'\n",
    "])\n",
    "    \n",
    "    # Load augmented data\n",
    "    file4 = pd.read_csv('/Users/rachael/Desktop/data/augmented_train0.txt', delimiter='\\t', header=None, names=['sentiment', 'content'])\n",
    "    file5 = pd.read_csv('/Users/rachael/Desktop/data/augmented_train1.txt', delimiter='\\t', header=None, names=['sentiment', 'content'])\n",
    "    file6 = pd.read_csv('/Users/rachael/Desktop/data/augmented_train2.txt', delimiter='\\t', header=None, names=['sentiment', 'content'])\n",
    "    augmented_train = pd.concat([file4,file5,file6])\n",
    "\n",
    "    #feature extraction for training data\n",
    "    #augmented_train['content'] = augmented_train['content'].astype(str)\n",
    "    augmented_train['content'] = augmented_train['content'].astype(str)\n",
    "    train_sentences = [item for item in list(augmented_train['content'])]\n",
    "    train_labels = [int(item) for item in list(augmented_train['sentiment'])]\n",
    "    X_train_tokenized = [tokenize(sentence) for sentence in train_sentences]\n",
    "    train_vectors = np.array([sentence_to_avg_vec(sentence,ft) for sentence in X_train_tokenized])\n",
    "\n",
    "    ##prepare test data for models\n",
    "    test_df['content'] = test_df['content'].astype(str)\n",
    "    test_sentences = [item for item in list(test_df['content'])]\n",
    "    test_labels = [int(item) for item in list(test_df['sentiment'])]\n",
    "    X_test_tokenized = [tokenize(sentence) for sentence in test_sentences]\n",
    "    test_vectors = np.array([sentence_to_avg_vec(sentence,ft) for sentence in X_test_tokenized])\n",
    "    #feature extraction for test data\n",
    "\n",
    "    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,  # Number of parameter settings that are sampled\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        cv=inner_cv,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit Randomized Search\n",
    "    randomized_search.fit(train_vectors,train_labels)\n",
    "\n",
    "    # Get the best model from Randomized Search\n",
    "    best_model = randomized_search.best_estimator_\n",
    "    best_params = randomized_search.best_params_\n",
    "    best_score = randomized_search.best_score_\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_predictions = best_model.predict(test_vectors)\n",
    "    test_f1_score = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    f1_scores.append(test_f1_score)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    print(f\"Fold F1-score: {test_f1_score}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Summarize the results\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "std_f1_score = np.std(f1_scores)\n",
    "\n",
    "print(f\"Mean F1-score: {mean_f1_score}\")\n",
    "print(f\"Standard deviation of F1-scores: {std_f1_score}\")\n",
    "print(\"Best parameters for each fold:\")\n",
    "for i, params in enumerate(best_params_list):\n",
    "    print(f\"Fold {i + 1}: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common best parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': 11, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "best_params_counter = Counter(tuple(sorted(params.items())) for params in best_params_list)\n",
    "most_common_params = dict(best_params_counter.most_common(1)[0][0])\n",
    "\n",
    "print(\"Most common best parameters:\", most_common_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_params = {'n_estimators': 46, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 28, 'criterion': 'gini', 'bootstrap': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(bootstrap=False, class_weight=&#x27;balanced&#x27;, max_depth=28,\n",
       "                       min_samples_split=8, n_estimators=46, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(bootstrap=False, class_weight=&#x27;balanced&#x27;, max_depth=28,\n",
       "                       min_samples_split=8, n_estimators=46, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=28,\n",
       "                       min_samples_split=8, n_estimators=46, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sentences = [item for item in list(X)]\n",
    "X_tokenized = X.apply(tokenize)\n",
    "X_embeddings = np.array([sentence_to_avg_vec(sentence,ft) for sentence in X_tokenized])\n",
    "\n",
    "final_model = RandomForestClassifier(random_state=42,**most_common_params,class_weight='balanced')\n",
    "final_model.fit(X_embeddings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.63      0.97      0.76       245\n",
      "           2       0.80      0.16      0.27       124\n",
      "\n",
      "    accuracy                           0.63       407\n",
      "   macro avg       0.48      0.38      0.34       407\n",
      "weighted avg       0.62      0.63      0.54       407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_excel('/Users/rachael/Downloads/test_df_labelled.xlsx')\n",
    "#the data verified by the third person will be used as the train set finally\n",
    "test = test[['content','sentiment']]\n",
    "test['content'] = test['content'].apply(remove_urls) \n",
    "test['content'] = test['content'].apply(clean_text) \n",
    "test['sentiment'] = test['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n",
    "test_tokenized = test['content'].apply(tokenize)\n",
    "test_embedding = np.array([sentence_to_avg_vec(sentence,ft) for sentence in test_tokenized])\n",
    "test_predictions = final_model.predict(test_embedding)\n",
    "test_score = accuracy_score(test['sentiment'], test_predictions)\n",
    "conf_matrix = confusion_matrix(test['sentiment'], test_predictions)\n",
    "class_report = classification_report(test['sentiment'], test_predictions)\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
