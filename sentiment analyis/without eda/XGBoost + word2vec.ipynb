{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, classification_report\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import accuracy_score,f1_score,balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold,RandomizedSearchCV\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each label_sentiment:\n",
      "sentiment\n",
      "Negative    262\n",
      "Neither      99\n",
      "Positive     48\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions of each label_sentiment:\n",
      "sentiment\n",
      "Negative    0.640587\n",
      "Neither     0.242054\n",
      "Positive    0.117359\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('/Users/rachael/Downloads/train_df_labelled copy.xlsx')\n",
    "#the data verified by the third person will be used as the train set finally\n",
    "file =  data[['content','sentiment']]\n",
    "file['sentiment'].value_counts()\n",
    "label_sentiment= file['sentiment'].value_counts()\n",
    "label_proportions_sentiment= file['sentiment'].value_counts(normalize=True)\n",
    "print(\"Counts of each label_sentiment:\")\n",
    "print(label_sentiment)\n",
    "print(\"\\nProportions of each label_sentiment:\")\n",
    "print(label_proportions_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Om het nog bondiger te zeggen: ik stemde ooit ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volgens de peilingen stond PVV 2 weken geleden...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PVV stemmers mogen dan het geheugen hebben van...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PVV wil geen vluchtelingen, Omtzigt wil geen e...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dat weten we niet. PVV heeft veel strategische...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Ik heb vvd gestemd omdat ik een coalitie over ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Exact, wordt niet bien van het krakeel en het ...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>De hysterische gedoe van nu kan wel later voor...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Als je het enkel hebt over de persoon van der ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Zoek het hoefijzermodel eens op, dan zie je da...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content sentiment\n",
       "0    Om het nog bondiger te zeggen: ik stemde ooit ...  Negative\n",
       "1    Volgens de peilingen stond PVV 2 weken geleden...  Negative\n",
       "2    PVV stemmers mogen dan het geheugen hebben van...  Negative\n",
       "3    PVV wil geen vluchtelingen, Omtzigt wil geen e...   Neither\n",
       "4    Dat weten we niet. PVV heeft veel strategische...   Neither\n",
       "..                                                 ...       ...\n",
       "404  Ik heb vvd gestemd omdat ik een coalitie over ...  Negative\n",
       "405  Exact, wordt niet bien van het krakeel en het ...   Neither\n",
       "406  De hysterische gedoe van nu kan wel later voor...  Negative\n",
       "407  Als je het enkel hebt over de persoon van der ...  Positive\n",
       "408  Zoek het hoefijzermodel eens op, dan zie je da...   Neither\n",
       "\n",
       "[409 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    # 定义URL的正则表达式\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    # 使用re.sub()函数替换URL为空字符串\n",
    "    no_url_text = re.sub(url_pattern, '', text)\n",
    "    return no_url_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove anything that is not a letter or space\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Optional: Convert text to lower case\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_898/3299540989.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['content'] = file['content'].apply(remove_urls)\n"
     ]
    }
   ],
   "source": [
    "file['content'] = file['content'].apply(remove_urls) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Om het nog bondiger te zeggen: ik stemde ooit ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volgens de peilingen stond PVV 2 weken geleden...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PVV stemmers mogen dan het geheugen hebben van...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PVV wil geen vluchtelingen, Omtzigt wil geen e...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dat weten we niet. PVV heeft veel strategische...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Ik heb vvd gestemd omdat ik een coalitie over ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Exact, wordt niet bien van het krakeel en het ...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>De hysterische gedoe van nu kan wel later voor...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Als je het enkel hebt over de persoon van der ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Zoek het hoefijzermodel eens op, dan zie je da...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content sentiment\n",
       "0    Om het nog bondiger te zeggen: ik stemde ooit ...  Negative\n",
       "1    Volgens de peilingen stond PVV 2 weken geleden...  Negative\n",
       "2    PVV stemmers mogen dan het geheugen hebben van...  Negative\n",
       "3    PVV wil geen vluchtelingen, Omtzigt wil geen e...   Neither\n",
       "4    Dat weten we niet. PVV heeft veel strategische...   Neither\n",
       "..                                                 ...       ...\n",
       "404  Ik heb vvd gestemd omdat ik een coalitie over ...  Negative\n",
       "405  Exact, wordt niet bien van het krakeel en het ...   Neither\n",
       "406  De hysterische gedoe van nu kan wel later voor...  Negative\n",
       "407  Als je het enkel hebt over de persoon van der ...  Positive\n",
       "408  Zoek het hoefijzermodel eens op, dan zie je da...   Neither\n",
       "\n",
       "[409 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_898/964840669.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['content'] = file['content'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file['content'] = file['content'].apply(clean_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>om het nog bondiger te zeggen ik stemde ooit l...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volgens de peilingen stond pvv  weken geleden ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pvv stemmers mogen dan het geheugen hebben van...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pvv wil geen vluchtelingen omtzigt wil geen ex...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dat weten we niet pvv heeft veel strategische ...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>ik heb vvd gestemd omdat ik een coalitie over ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>exact wordt niet bien van het krakeel en het m...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>de hysterische gedoe van nu kan wel later voor...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>als je het enkel hebt over de persoon van der ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>zoek het hoefijzermodel eens op dan zie je dat...</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content sentiment\n",
       "0    om het nog bondiger te zeggen ik stemde ooit l...  Negative\n",
       "1    volgens de peilingen stond pvv  weken geleden ...  Negative\n",
       "2    pvv stemmers mogen dan het geheugen hebben van...  Negative\n",
       "3    pvv wil geen vluchtelingen omtzigt wil geen ex...   Neither\n",
       "4    dat weten we niet pvv heeft veel strategische ...   Neither\n",
       "..                                                 ...       ...\n",
       "404  ik heb vvd gestemd omdat ik een coalitie over ...  Negative\n",
       "405  exact wordt niet bien van het krakeel en het m...   Neither\n",
       "406  de hysterische gedoe van nu kan wel later voor...  Negative\n",
       "407  als je het enkel hebt over de persoon van der ...  Positive\n",
       "408  zoek het hoefijzermodel eens op dan zie je dat...   Neither\n",
       "\n",
       "[409 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_5782/3999237305.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['sentiment'] = file['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n"
     ]
    }
   ],
   "source": [
    "file['sentiment'] = file['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n",
    "y = file['sentiment']\n",
    "X = file['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      om het nog bondiger te zeggen ik stemde ooit l...\n",
       "1      volgens de peilingen stond pvv  weken geleden ...\n",
       "2      pvv stemmers mogen dan het geheugen hebben van...\n",
       "3      pvv wil geen vluchtelingen omtzigt wil geen ex...\n",
       "4      dat weten we niet pvv heeft veel strategische ...\n",
       "                             ...                        \n",
       "404    ik heb vvd gestemd omdat ik een coalitie over ...\n",
       "405    exact wordt niet bien van het krakeel en het m...\n",
       "406    de hysterische gedoe van nu kan wel later voor...\n",
       "407    als je het enkel hebt over de persoon van der ...\n",
       "408    zoek het hoefijzermodel eens op dan zie je dat...\n",
       "Name: content, Length: 409, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "#load word2vec embeddings\n",
    "model_path = '/Users/rachael/Downloads/39/model.bin'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "vector_size = word_vectors.vector_size #100\n",
    "#convert sentences feature to the average of word vectors\n",
    "def sentence_to_avg_vec(tokens, model, vector_size):\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            vec += model[token]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg_vec(tokens, word_vectors, vector_size):\n",
    "    return np.mean([word_vectors.get(token, np.zeros(vector_size)) for token in tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameter range \n",
    "param_dist = {\n",
    "    'learning_rate': [item/100 for item in range(1, 31)],\n",
    "    'max_depth': [item for item in range(3, 11)],\n",
    "    'subsample': [item/10 for item in range(5, 11)],\n",
    "    'colsample_bytree': [item/10 for item in range(5, 11)],\n",
    "    'lambda': [item/10 for item in range(0, 11)],\n",
    "    'alpha': [item/10 for item in range(0, 11)],\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "test_scores = []\n",
    "best_params_list = []\n",
    "f1_scores = []\n",
    "balanced_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      om het nog bondiger te zeggen ik stemde ooit l...\n",
       "1      volgens de peilingen stond pvv  weken geleden ...\n",
       "2      pvv stemmers mogen dan het geheugen hebben van...\n",
       "3      pvv wil geen vluchtelingen omtzigt wil geen ex...\n",
       "4      dat weten we niet pvv heeft veel strategische ...\n",
       "                             ...                        \n",
       "404    ik heb vvd gestemd omdat ik een coalitie over ...\n",
       "405    exact wordt niet bien van het krakeel en het m...\n",
       "406    de hysterische gedoe van nu kan wel later voor...\n",
       "407    als je het enkel hebt over de persoon van der ...\n",
       "408    zoek het hoefijzermodel eens op dan zie je dat...\n",
       "Name: content, Length: 409, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fold Balanced Accuracy: 0.27336860670194\n",
      "Best parameters: {'subsample': 0.9, 'max_depth': 8, 'learning_rate': 0.29, 'lambda': 0.2, 'colsample_bytree': 1.0, 'alpha': 0.2}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fold Balanced Accuracy: 0.4132505175983437\n",
      "Best parameters: {'subsample': 1.0, 'max_depth': 10, 'learning_rate': 0.3, 'lambda': 0.0, 'colsample_bytree': 1.0, 'alpha': 0.5}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fold Balanced Accuracy: 0.3942077934013417\n",
      "Best parameters: {'subsample': 0.5, 'max_depth': 9, 'learning_rate': 0.26, 'lambda': 0.6, 'colsample_bytree': 0.8, 'alpha': 0.9}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fold Balanced Accuracy: 0.4579365079365079\n",
      "Best parameters: {'subsample': 0.6, 'max_depth': 10, 'learning_rate': 0.25, 'lambda': 0.3, 'colsample_bytree': 0.8, 'alpha': 0.7}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Fold Balanced Accuracy: 0.3284457478005865\n",
      "Best parameters: {'subsample': 0.6, 'max_depth': 10, 'learning_rate': 0.3, 'lambda': 0.9, 'colsample_bytree': 0.8, 'alpha': 0.2}\n",
      "Mean Balanced Accuracy: 0.37344183468774395\n",
      "Standard deviation of Balanced Accuracies: 0.06510699636512535\n",
      "Best parameters for each fold:\n",
      "Fold 1: {'subsample': 0.9, 'max_depth': 8, 'learning_rate': 0.29, 'lambda': 0.2, 'colsample_bytree': 1.0, 'alpha': 0.2}\n",
      "Fold 2: {'subsample': 1.0, 'max_depth': 10, 'learning_rate': 0.3, 'lambda': 0.0, 'colsample_bytree': 1.0, 'alpha': 0.5}\n",
      "Fold 3: {'subsample': 0.5, 'max_depth': 9, 'learning_rate': 0.26, 'lambda': 0.6, 'colsample_bytree': 0.8, 'alpha': 0.9}\n",
      "Fold 4: {'subsample': 0.6, 'max_depth': 10, 'learning_rate': 0.25, 'lambda': 0.3, 'colsample_bytree': 0.8, 'alpha': 0.7}\n",
      "Fold 5: {'subsample': 0.6, 'max_depth': 10, 'learning_rate': 0.3, 'lambda': 0.9, 'colsample_bytree': 0.8, 'alpha': 0.2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 分层K折交叉验证\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# 循环每个折叠\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    trainX, testX = X.iloc[train_index], X.iloc[test_index]\n",
    "    trainy, testy = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # 确保传递字符串数据\n",
    "    trainX = trainX.astype(str)\n",
    "    testX = testX.astype(str)\n",
    "\n",
    "    # 进行tokenize\n",
    "    X_train_tokenized = trainX.apply(tokenize)\n",
    "    X_test_tokenized = testX.apply(tokenize)\n",
    "\n",
    "     #map word embedding to tokenized train and test data\n",
    "    X_train_embeddings = np.array([sentence_to_avg_vec(tokens, word_vectors, vector_size) for tokens in X_train_tokenized])\n",
    "    X_test_embeddings= np.array([sentence_to_avg_vec(tokens, word_vectors, vector_size) for tokens in X_test_tokenized])\n",
    "\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,  # 采样次数\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,  # 使用所有可用的核心\n",
    "        cv=inner_cv,\n",
    "        random_state=42,\n",
    "        verbose=1  # 设置详细程度\n",
    "    )\n",
    "\n",
    "    # 拟合随机搜索\n",
    "    randomized_search.fit(X_train_embeddings, trainy)\n",
    "\n",
    "    # 获取最佳模型\n",
    "    best_model = randomized_search.best_estimator_\n",
    "    best_params = randomized_search.best_params_\n",
    "    best_score = randomized_search.best_score_\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    test_predictions = best_model.predict(X_test_embeddings)\n",
    "    balanced_acc = f1_score(testy, test_predictions,average='macro')\n",
    "\n",
    "    # 存储结果\n",
    "    balanced_accuracies.append(balanced_acc)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    print(f\"Fold Balanced Accuracy: {balanced_acc}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# 汇总结果\n",
    "\n",
    "mean_balanced_acc = np.mean(balanced_accuracies)\n",
    "std_balanced_acc = np.std(balanced_accuracies)\n",
    "\n",
    "print(f\"Mean Balanced Accuracy: {mean_balanced_acc}\")\n",
    "print(f\"Standard deviation of Balanced Accuracies: {std_balanced_acc}\")\n",
    "print(\"Best parameters for each fold:\")\n",
    "for i, params in enumerate(best_params_list):\n",
    "    print(f\"Fold {i + 1}: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common best parameters: {'alpha': 0.2, 'colsample_bytree': 1.0, 'lambda': 0.2, 'learning_rate': 0.29, 'max_depth': 8, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "best_params_counter = Counter(tuple(sorted(params.items())) for params in best_params_list)\n",
    "most_common_params = dict(best_params_counter.most_common(1)[0][0])\n",
    "\n",
    "print(\"Most common best parameters:\", most_common_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_params = {'subsample': 0.6, 'max_depth': 10, 'learning_rate': 0.25, 'lambda': 0.3, 'colsample_bytree': 0.8, 'alpha': 0.7}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(alpha=0.7, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, lambda=0.3, learning_rate=0.25,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(alpha=0.7, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, lambda=0.3, learning_rate=0.25,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(alpha=0.7, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, lambda=0.3, learning_rate=0.25,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None, ...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sentences = [item for item in list(X)]\n",
    "X_tokenized = X.apply(tokenize)\n",
    "X_embeddings = np.array([sentence_to_avg_vec(tokens, word_vectors, 100) for tokens in X_tokenized])\n",
    "\n",
    "final_model = XGBClassifier(use_label_encoder=False, random_state=42,**most_common_params)\n",
    "final_model.fit(X_embeddings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_excel('/Users/rachael/Downloads/test_df_labelled.xlsx')\n",
    "#the data verified by the third person will be used as the train set finally\n",
    "test = test[['content','sentiment']]\n",
    "test['content'] = test['content'].apply(remove_urls) \n",
    "test['content'] = test['content'].apply(clean_text) \n",
    "test['sentiment'] = test['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n",
    "test_tokenized = test['content'].apply(tokenize)\n",
    "test_embedding = np.array([sentence_to_avg_vec(tokens,word_vectors,100) for tokens in test_tokenized])\n",
    "test_predictions = final_model.predict(test_embedding)\n",
    "test_score = accuracy_score(test['sentiment'], test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test['sentiment'], test_predictions)\n",
    "class_report = classification_report(test['sentiment'], test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.63      0.95      0.76       245\n",
      "           2       0.70      0.21      0.32       124\n",
      "\n",
      "    accuracy                           0.64       407\n",
      "   macro avg       0.45      0.39      0.36       407\n",
      "weighted avg       0.60      0.64      0.56       407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
