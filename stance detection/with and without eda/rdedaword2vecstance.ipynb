{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold,RandomizedSearchCV\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, classification_report\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.cli import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each label_stance:\n",
      "stance\n",
      "Aganist    186\n",
      "Neither    140\n",
      "Support     82\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions of each label_stance:\n",
      "stance\n",
      "Aganist    0.455882\n",
      "Neither    0.343137\n",
      "Support    0.200980\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('/Users/rachael/Downloads/train_df_labelled.xlsx')\n",
    "#the data verified by the third person will be used as the train set finally\n",
    "file =  data[['content','stance']]\n",
    "file['stance'].value_counts()\n",
    "label_stance= file['stance'].value_counts()\n",
    "label_proportions_stance= file['stance'].value_counts(normalize=True)\n",
    "print(\"Counts of each label_stance:\")\n",
    "print(label_stance)\n",
    "print(\"\\nProportions of each label_stance:\")\n",
    "print(label_proportions_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls(text):\n",
    "    # 定义URL的正则表达式\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    # 使用re.sub()函数替换URL为空字符串\n",
    "    no_url_text = re.sub(url_pattern, '', text)\n",
    "    return no_url_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove anything that is not a letter or space\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Optional: Convert text to lower case\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_3918/271919431.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['content'] = file['content'].apply(remove_urls)\n",
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_3918/271919431.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['content'] = file['content'].apply(clean_text)\n",
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_3918/271919431.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['stance'] = file['stance'].replace({'Support':0,'Aganist': 1, 'Neither': 2})\n"
     ]
    }
   ],
   "source": [
    "file['content'] = file['content'].apply(remove_urls) \n",
    "file['content'] = file['content'].apply(clean_text) \n",
    "file['stance'] = file['stance'].replace({'Support':0,'Aganist': 1, 'Neither': 2})\n",
    "y = file['stance']\n",
    "X = file['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import spacy\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "#tokenize\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "#load pre-trained word2vec embedding \n",
    "model_path = '/Users/rachael/Downloads/39/model.bin'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "vector_size = word_vectors.vector_size #dimension of embedding 100\n",
    "\n",
    "# word embedding mapping and convert sentences to the average of vectors\n",
    "def sentence_to_avg_vec(tokens, model, vector_size):\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            vec += model[token]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter range\n",
    "param_dist= {\n",
    "    'n_estimators': range(10, 101),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2, 51),\n",
    "    'min_samples_split': range(2, 11),\n",
    "    'min_samples_leaf': range(1, 11),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "test_scores = []\n",
    "best_params_list = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold test score: 0.5361111111111111\n",
      "Best parameters: {'n_estimators': 48, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'log2', 'max_depth': 32, 'criterion': 'entropy', 'bootstrap': True}\n",
      "Fold test score: 0.4391865079365079\n",
      "Best parameters: {'n_estimators': 24, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': 23, 'criterion': 'gini', 'bootstrap': True}\n",
      "Fold test score: 0.4613926571188039\n",
      "Best parameters: {'n_estimators': 48, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'log2', 'max_depth': 32, 'criterion': 'entropy', 'bootstrap': True}\n",
      "Fold test score: 0.47368421052631576\n",
      "Best parameters: {'n_estimators': 36, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': None, 'max_depth': 3, 'criterion': 'gini', 'bootstrap': True}\n",
      "Fold test score: 0.5041554357592094\n",
      "Best parameters: {'n_estimators': 48, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_features': 'log2', 'max_depth': 49, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Mean test score: 0.48290598449038963\n",
      "Standard deviation of test scores: 0.03388681654490388\n",
      "Best parameters for each fold:\n",
      "Fold 1: {'n_estimators': 48, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'log2', 'max_depth': 32, 'criterion': 'entropy', 'bootstrap': True}\n",
      "Fold 2: {'n_estimators': 24, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': 23, 'criterion': 'gini', 'bootstrap': True}\n",
      "Fold 3: {'n_estimators': 48, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'log2', 'max_depth': 32, 'criterion': 'entropy', 'bootstrap': True}\n",
      "Fold 4: {'n_estimators': 36, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': None, 'max_depth': 3, 'criterion': 'gini', 'bootstrap': True}\n",
      "Fold 5: {'n_estimators': 48, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_features': 'log2', 'max_depth': 49, 'criterion': 'entropy', 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Stratified K-fold for maintaining label distribution,shuffle=True确保每次迭代的数据划分不同\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    trainX, testX = X.iloc[train_index], X.iloc[test_index]\n",
    "    trainy, testy = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    X_train_tokenized = trainX.apply(tokenize)\n",
    "    X_test_tokenized = testX.apply(tokenize)\n",
    "\n",
    "    #map word embedding to train data\n",
    "    \n",
    "    X_train_embeddings = np.array([sentence_to_avg_vec(tokens, word_vectors, vector_size) for tokens in X_train_tokenized])\n",
    "    X_test_embeddings = np.array([sentence_to_avg_vec(tokens, word_vectors, vector_size) for tokens in X_test_tokenized])\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=42,class_weight='balanced')\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,  # Number of parameter settings that are sampled\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        cv=inner_cv,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Fit Randomized Search\n",
    "    randomized_search.fit(X_train_embeddings, trainy)\n",
    "\n",
    "    # Get the best model from Randomized Search\n",
    "    best_model = randomized_search.best_estimator_\n",
    "    best_params = randomized_search.best_params_\n",
    "    best_score = randomized_search.best_score_\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_predictions = best_model.predict(X_test_embeddings)\n",
    "    test_score = f1_score(testy, test_predictions,average='macro')\n",
    "\n",
    "    test_scores.append(test_score)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    print(f\"Fold test score: {test_score}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Summarize the results\n",
    "mean_test_score = np.mean(test_scores)\n",
    "std_test_score = np.std(test_scores)\n",
    "\n",
    "print(f\"Mean test score: {mean_test_score}\")\n",
    "print(f\"Standard deviation of test scores: {std_test_score}\")\n",
    "print(\"Best parameters for each fold:\")\n",
    "for i, params in enumerate(best_params_list):\n",
    "    print(f\"Fold {i + 1}: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common best parameters: {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 32, 'max_features': 'log2', 'min_samples_leaf': 7, 'min_samples_split': 4, 'n_estimators': 48}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "best_params_counter = Counter(tuple(sorted(params.items())) for params in best_params_list)\n",
    "most_common_params = dict(best_params_counter.most_common(1)[0][0])\n",
    "\n",
    "print(\"Most common best parameters:\", most_common_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_params = {'n_estimators': 48, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'log2', 'max_depth': 32, 'criterion': 'entropy', 'bootstrap': True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, criterion=&#x27;entropy&#x27;,\n",
       "                       max_depth=32, max_features=&#x27;log2&#x27;, min_samples_leaf=7,\n",
       "                       min_samples_split=4, n_estimators=48, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, criterion=&#x27;entropy&#x27;,\n",
       "                       max_depth=32, max_features=&#x27;log2&#x27;, min_samples_leaf=7,\n",
       "                       min_samples_split=4, n_estimators=48, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
       "                       max_depth=32, max_features='log2', min_samples_leaf=7,\n",
       "                       min_samples_split=4, n_estimators=48, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sentences = [item for item in list(X)]\n",
    "X_tokenized = X.apply(tokenize)\n",
    "X_embeddings = np.array([sentence_to_avg_vec(tokens, word_vectors, 100) for tokens in X_tokenized])\n",
    "final_model = RandomForestClassifier(random_state=42,class_weight = 'balanced',**most_common_params)\n",
    "final_model.fit(X_embeddings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.21      0.25        76\n",
      "           1       0.53      0.56      0.54       191\n",
      "           2       0.54      0.58      0.56       140\n",
      "\n",
      "    accuracy                           0.50       407\n",
      "   macro avg       0.45      0.45      0.45       407\n",
      "weighted avg       0.49      0.50      0.49       407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_excel('/Users/rachael/Downloads/test_df_labelled.xlsx')\n",
    "#process test data \n",
    "test = test[['content','stance']]\n",
    "test['content'] = test['content'].apply(remove_urls) \n",
    "test['content'] = test['content'].apply(clean_text) \n",
    "test['stance'] = test['stance'].replace({'Support':0,'Aganist': 1, 'Neither': 2})\n",
    "test_tokenized = test['content'].apply(tokenize)\n",
    "test_embedding = np.array([sentence_to_avg_vec(tokens,word_vectors,100) for tokens in test_tokenized])\n",
    "\n",
    "#predict \n",
    "test_predictions = final_model.predict(test_embedding)\n",
    "conf_matrix = confusion_matrix(test['stance'], test_predictions)\n",
    "class_report = classification_report(test['stance'], test_predictions)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=6\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=2\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=3\n",
      "Fold F1-score: 0.39899665551839464\n",
      "Best parameters: {'n_estimators': 56, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 28, 'criterion': 'entropy', 'bootstrap': False}\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=6\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=2\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=3\n",
      "Fold F1-score: 0.5071607452317407\n",
      "Best parameters: {'n_estimators': 77, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 41, 'criterion': 'gini', 'bootstrap': False}\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=6\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=2\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=3\n",
      "Fold F1-score: 0.42422805580700323\n",
      "Best parameters: {'n_estimators': 77, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 41, 'criterion': 'gini', 'bootstrap': False}\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=6\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=2\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=3\n",
      "Fold F1-score: 0.5059952370403189\n",
      "Best parameters: {'n_estimators': 56, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 28, 'criterion': 'entropy', 'bootstrap': False}\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_0.txt to /Users/rachael/Desktop/data/augmented_train0.txt with num_aug=6\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_1.txt to /Users/rachael/Desktop/data/augmented_train1.txt with num_aug=2\n",
      "generated augmented sentences with eda for /Users/rachael/Desktop/data/sentiment_2.txt to /Users/rachael/Desktop/data/augmented_train2.txt with num_aug=3\n",
      "Fold F1-score: 0.4587134502923977\n",
      "Best parameters: {'n_estimators': 56, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 28, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Mean F1-score: 0.45901882877797107\n",
      "Standard deviation of F1-scores: 0.04321476478198628\n",
      "Best parameters for each fold:\n",
      "Fold 1: {'n_estimators': 56, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 28, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Fold 2: {'n_estimators': 77, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 41, 'criterion': 'gini', 'bootstrap': False}\n",
      "Fold 3: {'n_estimators': 77, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 41, 'criterion': 'gini', 'bootstrap': False}\n",
      "Fold 4: {'n_estimators': 56, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 28, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Fold 5: {'n_estimators': 56, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 28, 'criterion': 'entropy', 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Stratified K-fold for maintaining label distribution, shuffle=True ensures different data splits in each iteration\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42,class_weight = 'balanced')\n",
    "randomized_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,  # Number of parameter settings that are sampled\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        cv=inner_cv,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Fit Randomized Search\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    trainX, testX = X.iloc[train_index], X.iloc[test_index]\n",
    "    trainy, testy = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # 将 y_train 和 y_test 转换为 DataFrame\n",
    "    y_train_df = pd.DataFrame(trainy).reset_index(drop=True)\n",
    "    y_test_df = pd.DataFrame(testy).reset_index(drop=True)\n",
    "\n",
    "    # 拼接 X_train 和 y_train 以及 X_test 和 y_test\n",
    "    train_df = pd.concat([trainX.reset_index(drop=True), y_train_df], axis=1)\n",
    "    test_df = pd.concat([testX.reset_index(drop=True), y_test_df], axis=1)\n",
    "\n",
    "    # 标准化处理文本\n",
    "    train_df['content'] = train_df['content'].apply(lambda x: ' '.join(x.split()))\n",
    "    sentiment_0 = train_df[train_df['stance'] == 0]\n",
    "    sentiment_1 = train_df[train_df['stance'] == 1]\n",
    "    sentiment_2 = train_df[train_df['stance'] == 2]\n",
    "\n",
    "    # 数据增强\n",
    "    output_file_path0 = '/Users/rachael/Desktop/data/sentiment_0.txt'\n",
    "    with open(output_file_path0, 'w', encoding='utf-8') as f:\n",
    "        for _, row in sentiment_0.iterrows():\n",
    "            f.write(f\"{row['stance']}\\t{row['content']}\\n\")\n",
    "\n",
    "    subprocess.run([\n",
    "        'python', '/Users/rachael/Desktop/data/augment.py',\n",
    "        '--input', output_file_path0,\n",
    "        '--output', '/Users/rachael/Desktop/data/augmented_train0.txt',\n",
    "        '--num_aug', '6',\n",
    "        '--alpha_sr', '0.1',\n",
    "        '--alpha_rd', '0.1',\n",
    "        '--alpha_ri', '0.1',\n",
    "        '--alpha_rs', '0.1'\n",
    "    ])\n",
    "\n",
    "    output_file_path1 = '/Users/rachael/Desktop/data/sentiment_1.txt'\n",
    "    with open(output_file_path1, 'w', encoding='utf-8') as f:\n",
    "        for _, row in sentiment_1.iterrows():\n",
    "            f.write(f\"{row['stance']}\\t{row['content']}\\n\")\n",
    "\n",
    "    subprocess.run([\n",
    "        'python', '/Users/rachael/Desktop/data/augment.py',\n",
    "        '--input', output_file_path1,\n",
    "        '--output', '/Users/rachael/Desktop/data/augmented_train1.txt',\n",
    "        '--num_aug', '2',\n",
    "        '--alpha_sr', '0.1',\n",
    "        '--alpha_rd', '0.1',\n",
    "        '--alpha_ri', '0.1',\n",
    "        '--alpha_rs', '0.1'\n",
    "    ])\n",
    "\n",
    "    output_file_path2 = '/Users/rachael/Desktop/data/sentiment_2.txt'\n",
    "    with open(output_file_path2, 'w', encoding='utf-8') as f:\n",
    "        for _, row in sentiment_2.iterrows():\n",
    "            f.write(f\"{row['stance']}\\t{row['content']}\\n\")\n",
    "\n",
    "    subprocess.run([\n",
    "        'python', '/Users/rachael/Desktop/data/augment.py',\n",
    "        '--input', output_file_path2,\n",
    "        '--output', '/Users/rachael/Desktop/data/augmented_train2.txt',\n",
    "        '--num_aug', '3',\n",
    "        '--alpha_sr', '0.1',\n",
    "        '--alpha_rd', '0.1',\n",
    "        '--alpha_ri', '0.1',\n",
    "        '--alpha_rs', '0.1'\n",
    "    ])\n",
    "\n",
    "    # Load augmented data\n",
    "    file4 = pd.read_csv('/Users/rachael/Desktop/data/augmented_train0.txt', delimiter='\\t', header=None, names=['stance', 'content'])\n",
    "    file5 = pd.read_csv('/Users/rachael/Desktop/data/augmented_train1.txt', delimiter='\\t', header=None, names=['stance', 'content'])\n",
    "    file6 = pd.read_csv('/Users/rachael/Desktop/data/augmented_train2.txt', delimiter='\\t', header=None, names=['stance', 'content'])\n",
    "    augmented_train = pd.concat([file4, file5, file6])\n",
    "\n",
    "    #process train data\n",
    "    augmented_train['content'] = augmented_train['content'].astype(str)\n",
    "    y_train = augmented_train['stance']\n",
    "    X_train = augmented_train['content']\n",
    "    X_train_tokenized = X_train.apply(tokenize)\n",
    "    X_train_embeddings = np.array([sentence_to_avg_vec(tokens, word_vectors, vector_size) for tokens in X_train_tokenized])\n",
    "    #X_train_embeddings = scaler.fit_transform(X_train_embeddings1)\n",
    "\n",
    "    #prepare test data\n",
    "    test_df['content'] = test_df['content'].astype(str)\n",
    "    y_test = test_df['stance']\n",
    "    X_test = test_df['content']\n",
    "    X_test_tokenized = X_test.apply(tokenize)\n",
    "    X_test_embeddings = np.array([sentence_to_avg_vec(tokens, word_vectors, vector_size) for tokens in X_test_tokenized])\n",
    "\n",
    "    # Fit Randomized Search\n",
    "    randomized_search.fit(X_train_embeddings, y_train)\n",
    "\n",
    "    # Get the best model from Randomized Search\n",
    "    best_model = randomized_search.best_estimator_\n",
    "    best_params = randomized_search.best_params_\n",
    "    best_score = randomized_search.best_score_\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_predictions = best_model.predict(X_test_embeddings)\n",
    "    test_f1_score = f1_score(y_test, test_predictions, average='macro')\n",
    "\n",
    "    f1_scores.append(test_f1_score)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    print(f\"Fold F1-score: {test_f1_score}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Summarize the results\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "std_f1_score = np.std(f1_scores)\n",
    "\n",
    "print(f\"Mean F1-score: {mean_f1_score}\")\n",
    "print(f\"Standard deviation of F1-scores: {std_f1_score}\")\n",
    "print(\"Best parameters for each fold:\")\n",
    "for i, params in enumerate(best_params_list):\n",
    "    print(f\"Fold {i + 1}: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common best parameters: {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 28, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 6, 'n_estimators': 56}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "best_params_counter = Counter(tuple(sorted(params.items())) for params in best_params_list)\n",
    "most_common_params = dict(best_params_counter.most_common(1)[0][0])\n",
    "\n",
    "print(\"Most common best parameters:\", most_common_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_params = {'n_estimators': 77, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 41, 'criterion': 'gini', 'bootstrap': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(bootstrap=False, class_weight=&#x27;balanced&#x27;, max_depth=41,\n",
       "                       min_samples_split=7, n_estimators=77, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(bootstrap=False, class_weight=&#x27;balanced&#x27;, max_depth=41,\n",
       "                       min_samples_split=7, n_estimators=77, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=41,\n",
       "                       min_samples_split=7, n_estimators=77, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sentences = [item for item in list(X)]\n",
    "X_tokenized = X.apply(tokenize)\n",
    "X_embeddings = np.array([sentence_to_avg_vec(tokens, word_vectors, 100) for tokens in X_tokenized])\n",
    "final_model = RandomForestClassifier(random_state=42,class_weight = 'balanced',**most_common_params)\n",
    "final_model.fit(X_embeddings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.16      0.21        76\n",
      "           1       0.54      0.71      0.61       191\n",
      "           2       0.61      0.53      0.57       140\n",
      "\n",
      "    accuracy                           0.54       407\n",
      "   macro avg       0.49      0.46      0.46       407\n",
      "weighted avg       0.53      0.54      0.52       407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_excel('/Users/rachael/Downloads/test_df_labelled.xlsx')\n",
    "#process test data \n",
    "test = test[['content','stance']]\n",
    "test['content'] = test['content'].apply(remove_urls) \n",
    "test['content'] = test['content'].apply(clean_text) \n",
    "test['stance'] = test['stance'].replace({'Support':0,'Aganist': 1, 'Neither': 2})\n",
    "test_tokenized = test['content'].apply(tokenize)\n",
    "test_embedding = np.array([sentence_to_avg_vec(tokens,word_vectors,100) for tokens in test_tokenized])\n",
    "\n",
    "#predict \n",
    "test_predictions = final_model.predict(test_embedding)\n",
    "conf_matrix = confusion_matrix(test['stance'], test_predictions)\n",
    "class_report = classification_report(test['stance'], test_predictions)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4658275269325145\n"
     ]
    }
   ],
   "source": [
    "print(balanced_accuracy_score(test['stance'], test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
