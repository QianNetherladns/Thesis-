{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import fasttext\n",
    "import certifi\n",
    "import ssl\n",
    "import urllib.request\n",
    "import os\n",
    "import gzip\n",
    "import spacy\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, classification_report\n",
    "import subprocess\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each label_sentiment:\n",
      "sentiment\n",
      "Negative    262\n",
      "Neither      99\n",
      "Positive     48\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions of each label_sentiment:\n",
      "sentiment\n",
      "Negative    0.640587\n",
      "Neither     0.242054\n",
      "Positive    0.117359\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('/Users/rachael/Downloads/train_df_labelled copy.xlsx')\n",
    "#the data verified by the third person will be used as the train set finally\n",
    "file =  data[['content','sentiment']]\n",
    "file['sentiment'].value_counts()\n",
    "label_sentiment= file['sentiment'].value_counts()\n",
    "label_proportions_sentiment= file['sentiment'].value_counts(normalize=True)\n",
    "print(\"Counts of each label_sentiment:\")\n",
    "print(label_sentiment)\n",
    "print(\"\\nProportions of each label_sentiment:\")\n",
    "print(label_proportions_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls(text):\n",
    "    # the expression of URL_pattern\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    # use re.sub()to sustitude URL with blank str\n",
    "    no_url_text = re.sub(url_pattern, '', text)\n",
    "    return no_url_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove anything that is not a letter or space\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Optional: Convert text to lower case\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_5556/280474677.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['content'] = file['content'].apply(remove_urls)\n",
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_5556/280474677.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['content'] = file['content'].apply(clean_text)\n",
      "/var/folders/vq/5p2k6x290wzfc5dzz5xm78bm0000gp/T/ipykernel_5556/280474677.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file['sentiment'] = file['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n"
     ]
    }
   ],
   "source": [
    "#clean the data\n",
    "file['content'] = file['content'].apply(remove_urls) \n",
    "file['content'] = file['content'].apply(clean_text) \n",
    "\n",
    "#convert categorical to numerical \n",
    "file['sentiment'] = file['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n",
    "\n",
    "# Load dataset\n",
    "y = file['sentiment']\n",
    "X = file['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Path to your FastText pre-trained word embedding file\n",
    "fasttext_model_path = '/Users/rachael/Downloads/cc.nl.300.bin'\n",
    "\n",
    "# Load the FastText model\n",
    "ft = fasttext.load_model(fasttext_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "#tokenize\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def sentence_to_avg_vec(tokens, model):\n",
    "    vector_size = model.get_dimension()\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        vec += model.get_word_vector(token)\n",
    "        count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist= {\n",
    "    'n_estimators': range(10, 101),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2, 51),\n",
    "    'min_samples_split': range(2, 11),\n",
    "    'min_samples_leaf': range(1, 11),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "test_scores = []\n",
    "best_params_list = []\n",
    "\n",
    "# Stratified K-fold for maintaining label distribution,shuffle=True确保每次迭代的数据划分不同\n",
    "# Initialize parameters\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold test score: 0.3173333333333333\n",
      "Best parameters: {'n_estimators': 13, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 37, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Fold test score: 0.3448975188781014\n",
      "Best parameters: {'n_estimators': 74, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 47, 'criterion': 'gini', 'bootstrap': False}\n",
      "Fold test score: 0.341025641025641\n",
      "Best parameters: {'n_estimators': 46, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 3, 'criterion': 'entropy', 'bootstrap': True}\n",
      "Fold test score: 0.4333333333333333\n",
      "Best parameters: {'n_estimators': 16, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_depth': 46, 'criterion': 'gini', 'bootstrap': False}\n",
      "Fold test score: 0.3289593584836887\n",
      "Best parameters: {'n_estimators': 12, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 36, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Mean test score: 0.35310983701081955\n",
      "Standard deviation of test scores: 0.04126132124012548\n",
      "Best parameters for each fold:\n",
      "Fold 1: {'n_estimators': 13, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 37, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Fold 2: {'n_estimators': 74, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 47, 'criterion': 'gini', 'bootstrap': False}\n",
      "Fold 3: {'n_estimators': 46, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 3, 'criterion': 'entropy', 'bootstrap': True}\n",
      "Fold 4: {'n_estimators': 16, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_depth': 46, 'criterion': 'gini', 'bootstrap': False}\n",
      "Fold 5: {'n_estimators': 12, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 36, 'criterion': 'entropy', 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    trainX, testX = X.iloc[train_index], X.iloc[test_index]\n",
    "    trainy, testy = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    X_train_tokenized = trainX.apply(tokenize)\n",
    "    train_vectors = np.array([sentence_to_avg_vec(sentence,ft) for sentence in X_train_tokenized])\n",
    "\n",
    "    X_test_tokenized = testX.apply(tokenize)\n",
    "    test_vectors = np.array([sentence_to_avg_vec(sentence,ft) for sentence in X_test_tokenized])\n",
    "    #feature extraction for test data\n",
    "\n",
    "    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rf = RandomForestClassifier(random_state=42,class_weight='balanced')\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,  # Number of parameter settings that are sampled\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        cv=inner_cv,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit Randomized Search\n",
    "    randomized_search.fit(train_vectors,trainy)\n",
    "\n",
    "    # Get the best model from Randomized Search\n",
    "    best_model = randomized_search.best_estimator_\n",
    "    best_params = randomized_search.best_params_\n",
    "    best_score = randomized_search.best_score_\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_predictions = best_model.predict(test_vectors)\n",
    "    test_score =f1_score(testy, test_predictions,average='macro')\n",
    "    \n",
    "    test_scores.append(test_score)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    print(f\"Fold test score: {test_score}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Summarize the results\n",
    "mean_test_score = np.mean(test_scores)\n",
    "std_test_score = np.std(test_scores)\n",
    "\n",
    "print(f\"Mean test score: {mean_test_score}\")\n",
    "print(f\"Standard deviation of test scores: {std_test_score}\")\n",
    "print(\"Best parameters for each fold:\")\n",
    "for i, params in enumerate(best_params_list):\n",
    "    print(f\"Fold {i + 1}: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common best parameters: {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 37, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 13}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "best_params_counter = Counter(tuple(sorted(params.items())) for params in best_params_list)\n",
    "most_common_params = dict(best_params_counter.most_common(1)[0][0])\n",
    "\n",
    "print(\"Most common best parameters:\", most_common_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_params = {'n_estimators': 16, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_depth': 46, 'criterion': 'gini', 'bootstrap': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(bootstrap=False, class_weight=&#x27;balanced&#x27;, max_depth=46,\n",
       "                       min_samples_leaf=8, n_estimators=16, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(bootstrap=False, class_weight=&#x27;balanced&#x27;, max_depth=46,\n",
       "                       min_samples_leaf=8, n_estimators=16, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=46,\n",
       "                       min_samples_leaf=8, n_estimators=16, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sentences = [item for item in list(X)]\n",
    "X_tokenized = X.apply(tokenize)\n",
    "X_embeddings = np.array([sentence_to_avg_vec(sentence,ft) for sentence in X_tokenized])\n",
    "\n",
    "final_model = RandomForestClassifier(random_state=42,**most_common_params,class_weight='balanced')\n",
    "final_model.fit(X_embeddings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.13      0.16        38\n",
      "           1       0.66      0.82      0.73       245\n",
      "           2       0.59      0.39      0.47       124\n",
      "\n",
      "    accuracy                           0.62       407\n",
      "   macro avg       0.49      0.45      0.45       407\n",
      "weighted avg       0.60      0.62      0.60       407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_excel('/Users/rachael/Downloads/test_df_labelled.xlsx')\n",
    "#the data verified by the third person will be used as the train set finally\n",
    "test = test[['content','sentiment']]\n",
    "test['content'] = test['content'].apply(remove_urls) \n",
    "test['content'] = test['content'].apply(clean_text) \n",
    "test['sentiment'] = test['sentiment'].replace({'Positive':0,'Negative': 1, 'Neither': 2})\n",
    "test_tokenized = test['content'].apply(tokenize)\n",
    "test_embedding = np.array([sentence_to_avg_vec(sentence,ft) for sentence in test_tokenized])\n",
    "test_predictions = final_model.predict(test_embedding)\n",
    "test_score = accuracy_score(test['sentiment'], test_predictions)\n",
    "conf_matrix = confusion_matrix(test['sentiment'], test_predictions)\n",
    "class_report = classification_report(test['sentiment'], test_predictions)\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
